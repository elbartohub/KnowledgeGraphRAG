#!/usr/bin/env python3
"""
Demonstration script showing how extracted keywords become graph nodes
and how jieba handles query segmentation in the DeepMind RAG system.
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import networkx as nx
import jieba
import json
from collections import defaultdict

def create_sample_knowledge_graph(keywords):
    """Create a sample knowledge graph from extracted keywords"""
    G = nx.Graph()
    
    # Add nodes (keywords become graph nodes)
    for keyword in keywords:
        G.add_node(keyword, type='concept')
    
    # Create sample relationships based on co-occurrence or semantic similarity
    # In a real implementation, this would be based on document analysis
    sample_relationships = [
        ('é«˜å£“æ°§ç™‚æ³•', 'HBO', 'synonym'),
        ('ç³–å°¿ç—…', 'ä¸‹è‚¢æ½°ç˜', 'causes'),
        ('å¿ƒè‚Œèˆ’å¼µåŠŸèƒ½', 'å¿ƒè‚Œ', 'part_of'),
        ('è„ˆè¡æ³¢å¤šæ™®å‹’è¶…è²å¿ƒå‹•åœ–', 'å¿ƒè¡€ç®¡åŠŸèƒ½', 'measures'),
        ('é‹å‹•åº·å¾©', 'é‹å‹•å“¡', 'benefits'),
        ('DeepMind', 'AI', 'develops'),
        ('äººå·¥æ™ºæ…§', 'é†«å­¸', 'applied_to')
    ]
    
    # Add edges with relationship types
    for source, target, rel_type in sample_relationships:
        if source in keywords and target in keywords:
            G.add_edge(source, target, relationship=rel_type)
    
    return G

def demonstrate_query_processing():
    """Demonstrate how queries are processed with jieba segmentation"""
    
    print("ğŸ” QUERY PROCESSING DEMONSTRATION")
    print("=" * 60)
    
    # Load custom keywords into jieba
    custom_keywords = [
        "HBOæ²»ç™‚", "HBO æ²»ç™‚", "é«˜å£“æ°§æ²»ç™‚", "å¤šæ™®å‹’è¶…è²å¿ƒå‹•åœ–", 
        "DeepMind", "ç™½é–‹æ°´", "äººå·¥æ™ºæ…§", "æ©Ÿå™¨å­¸ç¿’"
    ]
    
    for keyword in custom_keywords:
        jieba.add_word(keyword, freq=1000)
    
    # Test queries
    test_queries = [
        "ä»€éº¼æ˜¯HBOæ²»ç™‚çš„åŸç†ï¼Ÿ",
        "DeepMindé–‹ç™¼äº†å“ªäº›AIæŠ€è¡“ï¼Ÿ",
        "å¤šæ™®å‹’è¶…è²å¿ƒå‹•åœ–å¦‚ä½•æª¢æ¸¬å¿ƒè‡ŸåŠŸèƒ½ï¼Ÿ",
        "é«˜å£“æ°§æ²»ç™‚å°ç³–å°¿ç—…æ‚£è€…æœ‰æ•ˆå—ï¼Ÿ",
        "ç™½é–‹æ°´å°èº«é«”æœ‰ä»€éº¼å¥½è™•ï¼Ÿ"
    ]
    
    print("ğŸ“ Query Segmentation Results:")
    for i, query in enumerate(test_queries, 1):
        segments = list(jieba.cut(query))
        print(f"  {i}. Query: {query}")
        print(f"     Segments: {' | '.join(segments)}")
        
        # Extract key terms for graph search
        key_terms = [seg for seg in segments if len(seg.strip()) > 1 and seg not in ['ï¼Ÿ', 'çš„', 'æ˜¯', 'äº†', 'æœ‰', 'åœ¨', 'å°', 'å¦‚ä½•', 'ä»€éº¼', 'å“ªäº›']]
        print(f"     Key terms for graph search: {key_terms}")
        print()

def main():
    """Main demonstration function"""
    
    print("ğŸ¯ DEEPMIND RAG SYSTEM - KEYWORD EXTRACTION & GRAPH DEMO")
    print("=" * 70)
    
    # Sample extracted keywords (from Gemini API processing)
    sample_keywords = [
        'é«˜å£“æ°§ç™‚æ³•', 'ç³–å°¿ç—…', 'å¿ƒè‚Œèˆ’å¼µåŠŸèƒ½', 'å¿ƒè‚Œ', 'HBO',
        'è„ˆè¡æ³¢å¤šæ™®å‹’è¶…è²å¿ƒå‹•åœ–', 'çµ„ç¹”å¤šæ™®å‹’è¶…è²å¿ƒå‹•åœ–', 'å·¦å¿ƒå®¤', 'äºŒå°–ç“£', 'å³å¿ƒå®¤',
        'é‹å‹•åº·å¾©', 'é‹å‹•å“¡', 'åº·å¾©é†«å­¸', 'ç‰©ç†æ²»ç™‚', 'çµ„ç¹”ä¿®å¾©',
        'DeepMind', 'AlphaGo', 'AlphaFold', 'AI', 'äººå·¥æ™ºæ…§',
        'æ©Ÿå™¨å­¸ç¿’', 'æ·±åº¦å­¸ç¿’', 'ç¥ç¶“ç¶²çµ¡', 'å¼·åŒ–å­¸ç¿’', 'è›‹ç™½è³ªæŠ˜ç–Š'
    ]
    
    print("ğŸ“Š STEP 1: DOCUMENT PROCESSING")
    print("-" * 40)
    print("âœ… Documents uploaded to /uploads folder")
    print("âœ… Gemini API extracts top 50 keywords per document")
    print(f"âœ… Example extracted keywords: {len(sample_keywords)} terms")
    print("   Sample: " + ", ".join(sample_keywords[:10]) + ", ...")
    
    # Create knowledge graph
    print(f"\nğŸ•¸ï¸ STEP 2: KNOWLEDGE GRAPH CREATION")
    print("-" * 40)
    G = create_sample_knowledge_graph(sample_keywords)
    print(f"âœ… Created knowledge graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges")
    print(f"âœ… Nodes represent extracted keywords")
    print(f"âœ… Edges represent relationships between concepts")
    
    # Show graph structure
    print(f"\nğŸ“ˆ Graph Structure Sample:")
    for i, (node1, node2, data) in enumerate(list(G.edges(data=True))[:5]):
        rel_type = data.get('relationship', 'related')
        print(f"   {node1} --[{rel_type}]--> {node2}")
    
    print(f"\nğŸ” STEP 3: QUERY PROCESSING")
    print("-" * 40)
    demonstrate_query_processing()
    
    print("ğŸ’¡ SYSTEM WORKFLOW SUMMARY:")
    print("=" * 70)
    print("1. ğŸ“„ Document Upload â†’ Gemini extracts keywords â†’ Graph nodes")
    print("2. ğŸ” User Query â†’ Jieba segments â†’ Search graph â†’ Retrieve docs")
    print("3. ğŸ¤– Found docs â†’ Gemini generates answer | No docs â†’ Fallback message")
    print()
    print("ğŸ¯ KEY BENEFITS:")
    print("   âœ… Separates document analysis (Gemini) from query processing (jieba)")
    print("   âœ… Graph nodes focus on document concepts")
    print("   âœ… Jieba preserves important query terms")
    print("   âœ… Handles both Chinese and English terminology")
    
if __name__ == "__main__":
    main()
